<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>谷歌：“我们没有壁垒，他们也没有” | JohnYin</title>
<link rel="shortcut icon" href="https://johnyin-hub.github.io//favicon.ico?v=1683505903280">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.3.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://johnyin-hub.github.io//styles/main.css">
<link rel="alternate" type="application/atom+xml" title="谷歌：“我们没有壁垒，他们也没有” | JohnYin - Atom Feed" href="https://johnyin-hub.github.io//atom.xml">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">



    <meta name="description" content="大模型没有壁垒
原文：Google &quot;We Have No Moat, And Neither Does OpenAI&quot;
引言
2023年5月4日一份谷歌内部资料泄露，表达大模型时代谷歌和其他公司都没有技术壁垒。
下面的..." />
    <meta name="keywords" content="" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
    <script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.5.1/build/highlight.min.js"></script>
  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="https://johnyin-hub.github.io/">
  <img class="avatar" src="https://johnyin-hub.github.io//images/avatar.png?v=1683505903280" alt="">
  </a>
  <h1 class="site-title">
    JohnYin
  </h1>
  <p class="site-description">
    看代码、看论文、看乐谱真挺无聊，还是好好看看身边人有意思
  </p>
  <div class="menu-container">
    
      
        <a href="/" class="menu">
          首页
        </a>
      
    
      
        <a href="/archives" class="menu">
          归档
        </a>
      
    
      
        <a href="/tags" class="menu">
          标签
        </a>
      
    
      
        <a href="/post/about" class="menu">
          关于
        </a>
      
    
      
        <a href="心路记忆" class="menu">
          心路记忆
        </a>
      
    
  </div>
  <div class="social-container">
    
      
    
      
    
      
    
      
    
      
    
  </div>
</div>

        <div class="post-detail">
          <article class="post">
            <h2 class="post-title">
              谷歌：“我们没有壁垒，他们也没有”
            </h2>
            <div class="post-info">
              <span>
                2023-05-06
              </span>
              <span>
                11 min read
              </span>
              
            </div>
            
            <div class="post-content-wrapper">
              <div class="post-content" v-pre>
                <h1 id="大模型没有壁垒">大模型没有壁垒</h1>
<p>原文：<a href="https://www.semianalysis.com/p/google-we-have-no-moat-and-neither">Google &quot;We Have No Moat, And Neither Does OpenAI&quot;</a></p>
<h2 id="引言">引言</h2>
<p>2023年5月4日一份谷歌内部资料泄露，表达大模型时代谷歌和其他公司都没有技术壁垒。</p>
<p><em>下面的文本是一份最近泄露的文件，由一位匿名人士在公共 Discord 服务器上共享，该服务器已授予其重新发布的许可。它起源于谷歌内部的一名研究员。我们已经验证了它的真实性。唯一的修改是格式化和删除指向内部网页的链接。该文件仅代表谷歌员工的意见，并非整个公司的意见。我们不同意下面写的内容，我们询问的其他研究人员也不同意，但我们将在另一篇文章中发表我们对此的看法，供订阅者使用。我们只是分享这份文件的工具，这份文件提出了一些非常有趣的观点。</em></p>
<p>ChatGPT席卷全球，通用人工智能的曙光正在来临。然而这场人工智能的变革到底将以什么样的形式进行，谁终究能掌握技术的主动权？答案揭晓于现实世界中，这篇文章可以带来技术、商业和市场化的思考。包括我在22年3月以前都只认为这次的变革会受寡头绝对优势的垄断，但很明显技术的发展并不像我想象的那样，我自己也可以微调测试多个大模型了。</p>
<h2 id="complain">Complain</h2>
<p><strong>作者首先认为主流公司没有办法在变革中占到绝对优势，主要的开源问题已经被解决，且技术掌握在大众手里。原文如下：</strong></p>
<p>I’m talking, of course, about open source. Plainly put, they are lapping us. <strong>Things we consider “major open problems” are solved and in people’s hands today.</strong> Just to name a few:</p>
<ul>
<li><strong>LLMs on a Phone:</strong> <a href="https://twitter.com/thiteanish/status/1635678053853536256">People are running foundation models on a Pixel 6</a> at 5 tokens / sec.</li>
<li><strong>Scalable Personal AI:</strong> <a href="https://github.com/tloen/alpaca-lora">You can finetune a personalized AI on your laptop in an evening.</a></li>
<li><strong>Responsible Release:</strong> This one isn’t “solved” so much as “obviated”. <a href="https://civitai.com/">There are entire websites full of art models with no restrictions whatsoever</a>, and text is <a href="https://medium.com/geekculture/list-of-open-sourced-fine-tuned-large-language-models-llm-8d95a2e0dc76">not far behind.</a></li>
<li><strong>Multimodality:</strong> <a href="https://arxiv.org/pdf/2303.16199.pdf">The current multimodal ScienceQA SOTA was trained in an hour</a>.</li>
</ul>
<ol>
<li>大模型在移动设备的运行。</li>
<li>扩展个性化AI，每个人都可以参与微调。</li>
<li>不需要过多承担风险责任问题</li>
<li>大模型多模态问题。</li>
</ol>
<p>然后作者吐槽了斥巨资训练的超大模型在质量上对比开源免费的模型并占不到便宜。目前主流模型效果如下图：</p>
<figure data-type="image" tabindex="1"><img src="https://johnyin.oss-cn-shanghai.aliyuncs.com/uPic/image-20230506030004466.png" alt="image-20230506030004466" loading="lazy"></figure>
<h2 id="reason">Reason</h2>
<p>作者直接了当的指出Meta公司开放的LAMA模型让公众有能力具备大模型基础技术原文：</p>
<p>At the beginning of March the open source community <a href="https://www.vice.com/en/article/xgwqgw/facebooks-powerful-large-language-model-leaks-online-4chan-llama">got their hands on</a> their first really capable foundation model, as Meta’s LLaMA was leaked to the public. It had no instruction or conversation tuning, and no RLHF. Nonetheless, the community immediately understood the significance of what they had been given.</p>
<p>此外作者认识到开源社区创新的能力是他们无法想象的，包括一些指令微调、量化优化、人工评估、RLHF、多模态等。原文：</p>
<p>A tremendous outpouring of innovation followed, with just days between major developments (see The Timeline for the full breakdown). Here we are, barely a month later, and there are variants with <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">instruction tuning</a>, <a href="https://github.com/ggerganov/llama.cpp">quantization</a>, <a href="https://lmsys.org/blog/2023-03-30-vicuna/">quality improvements</a>, <a href="https://arxiv.org/pdf/2303.16199.pdf">human evals</a>, <a href="https://arxiv.org/pdf/2303.16199.pdf">multimodality</a>, <a href="https://drive.google.com/file/d/10iR5hKwFqAKhL3umx8muOWSRm7hs5FqX/view">RLHF</a>, etc. etc. many of which build on each other.</p>
<h2 id="why">Why</h2>
<p>​		作者拿图像的stable diffusion 的技术变革来类比这次大模型的变得，说的直白一点就是，开源的技术质量太好了，好到做闭源的没有明显优势了。</p>
<p>​		我也认为当开源技术效果好到一定地步的时候，将会滚雪球一样，无数的公司个人会争先的进入这个领域实现他们的想法。这样形成的产品、市场、用户交互的创新将是无法阻挡的。</p>
<h2 id="regret">Regret</h2>
<p>作者直接了当的夸奖了lora的微调技术，表示当初应该更关注lora这类微调技术。原文如下：</p>
<p>The innovations that powered open source’s recent successes directly solve problems we’re still struggling with. Paying more attention to their work could help us to avoid reinventing the wheel.</p>
<h4 id="lora-is-an-incredibly-powerful-technique-we-should-probably-be-paying-more-attention-to">LoRA is an incredibly powerful technique we should probably be paying more attention to</h4>
<p><a href="https://arxiv.org/abs/2106.09685">LoRA </a>works by representing model updates as low-rank factorizations, which reduces the size of the update matrices by a factor of up to several thousand. This allows model fine-tuning at a fraction of the cost and time. Being able to personalize a language model in a few hours on consumer hardware is a big deal, <em>particularly</em> for <a href="http://www.internalgooglesitescrubbedbyus.com/">aspirations that involve incorporating new and diverse knowledge in near real-time</a>. The fact that this technology exists is underexploited inside Google, even though it directly impacts some of our most ambitious projects.</p>
<h2 id="conclusion">Conclusion</h2>
<p>作者总结了很多结论性的东西，有的有冗余，我融合了一些传达清楚意思如下：</p>
<ol>
<li>从头训练一个大模型代价很高，如果小模型上能快速的升级改造且质量直逼大模型，那么大模型的优势几乎就没有。</li>
<li>数据质量比数据大小好。（我作为一个从业人员从来不怀疑这一点，感性的也可以感知大模型的“学话”过程和样本质量有较强的关联性。）</li>
<li>不应该和开源直接竞争，应该建立良好的开源系统。推动创新，控制创新。</li>
<li>个人和大公司开发模型应用承担的责任是不一样的，</li>
</ol>
<h2 id="epilogue">Epilogue</h2>
<p>作者在最后提到了OpenAI，认为openAI和谷歌一样遇到同样的问题，犯了同样的错误，预测OpenAI会被开源打败。</p>
<p>我部分同意上述观点，但OpenAI的广泛的用户测试积累了大量真实数据。即使没有技术全部开源，数据的优势只会更加明显。</p>
<h1 id="the-timeline">The Timeline</h1>
<p>最后作者放了疯狂3月的时间线。</p>
<p>在3月LAMA开放的时候并未意识到变革的到来，Timeline上的项目拉开了这场变革的序幕，我作为后知后觉的韭菜跑步进场希望分得这场变革的一杯羹。技术变革如此之快，快到未来5年的技术世界都不敢想象。我们如何才能在这场变革中抓到技术的主动权？通用人工智能的蓝图到底以何种形式展开？</p>
<p>变革时代，宁可后知后觉，不可不知不觉</p>
<h3 id="feb-24-2023-llama-is-launched">Feb 24, 2023 - LLaMA is Launched</h3>
<p><a href="https://ai.facebook.com/blog/large-language-model-llama-meta-ai/">Meta launches LLaMA</a>, open sourcing the code, but not the weights. At this point, LLaMA is not instruction or conversation tuned. Like many current models, it is a relatively small model (available at 7B, 13B, 33B, and 65B parameters) that has been trained for a relatively large amount of time, and is therefore quite capable relative to its size.</p>
<h3 id="march-3-2023-the-inevitable-happens">March 3, 2023 - The Inevitable Happens</h3>
<p>Within a week, <a href="https://www.vice.com/en/article/xgwqgw/facebooks-powerful-large-language-model-leaks-online-4chan-llama">LLaMA is leaked to the public</a>. The impact on the community cannot be overstated. Existing licenses prevent it from being used for commercial purposes, but suddenly anyone is able to experiment. From this point forward, innovations come hard and fast.</p>
<h3 id="march-12-2023-language-models-on-a-toaster">March 12, 2023 - Language models on a Toaster</h3>
<p>A little over a week later, Artem Andreenko <a href="https://github.com/ggerganov/llama.cpp/issues/58">gets the model working on a Raspberry Pi</a>. At this point the model runs too slowly to be practical because the weights must be paged in and out of memory. Nonetheless, this sets the stage for an onslaught of minification efforts.</p>
<h3 id="march-13-2023-fine-tuning-on-a-laptop">March 13, 2023 - Fine Tuning on a Laptop</h3>
<p>The next day, Stanford releases <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">Alpaca</a>, which adds instruction tuning to LLaMA. More important than the actual weights, however, was Eric Wang’s <a href="https://github.com/tloen/alpaca-lora">alpaca-lora</a> repo, which used <a href="https://arxiv.org/abs/2106.09685">low rank fine-tuning</a> to do this training “within hours on a single RTX 4090”.</p>
<p>Suddenly, anyone could fine-tune the model to do anything, kicking off a race to the bottom on low-budget fine-tuning projects. Papers proudly describe their total spend of a few hundred dollars. What’s more, the low rank updates can be distributed easily and separately from the original weights, making them independent of the original license from Meta. Anyone can share and apply them.</p>
<h3 id="march-18-2023-now-its-fast">March 18, 2023 - Now It’s Fast</h3>
<p>Georgi Gerganov <a href="https://github.com/ggerganov/llama.cpp">uses 4 bit quantization</a> to run LLaMA on a MacBook CPU. It is the first “no GPU” solution that is fast enough to be practical.</p>
<h3 id="march-19-2023-a-13b-model-achieves-parity-with-bard">March 19, 2023 - A 13B model achieves “parity” with Bard</h3>
<p>The next day, a cross-university collaboration releases <a href="https://lmsys.org/blog/2023-03-30-vicuna/">Vicuna</a>, and uses GPT-4-powered eval to provide qualitative comparisons of model outputs. While the evaluation method is suspect, the model is materially better than earlier variants. <strong>Training Cost: $300.</strong></p>
<p>Notably, they were able to use data from ChatGPT while circumventing restrictions on its API - They simply sampled examples of “impressive” ChatGPT dialogue posted on sites like <a href="https://sharegpt.com/">ShareGPT</a>.</p>
<h3 id="march-25-2023-choose-your-own-model">March 25, 2023 - Choose Your Own Model</h3>
<p>Nomic creates <a href="https://github.com/nomic-ai/gpt4all">GPT4All</a>, which is both a <a href="https://s3.amazonaws.com/static.nomic.ai/gpt4all/2023_GPT4All_Technical_Report.pdf">model </a>and, more importantly, an <a href="https://github.com/nomic-ai/gpt4all#gpt4all-compatibility-ecosystem">ecosystem</a>. For the first time, we see models (including Vicuna) being gathered together in one place. <strong>Training Cost: $100.</strong></p>
<h3 id="march-28-2023-open-source-gpt-3">March 28, 2023 - Open Source GPT-3</h3>
<p>Cerebras (not to be confused with our own Cerebra) trains the GPT-3 architecture using the optimal compute schedule implied by Chinchilla, and the optimal scaling implied by <a href="https://arxiv.org/abs/2203.03466">μ-parameterization</a>. This outperforms existing GPT-3 clones by a wide margin, and represents the first confirmed use of μ-parameterization “in the wild”. These models are trained from scratch, meaning the community is no longer dependent on LLaMA.</p>
<h3 id="march-28-2023-multimodal-training-in-one-hour">March 28, 2023 - Multimodal Training in One Hour</h3>
<p>Using a novel Parameter Efficient Fine Tuning (PEFT) technique, <a href="https://arxiv.org/pdf/2303.16199.pdf">LLaMA-Adapter</a> introduces instruction tuning and multimodality in one hour of training. Impressively, they do so with just 1.2M learnable parameters. The model achieves a new SOTA on multimodal ScienceQA.</p>
<h3 id="april-3-2023-real-humans-cant-tell-the-difference-between-a-13b-open-model-and-chatgpt">April 3, 2023 - Real Humans Can’t Tell the Difference Between a 13B Open Model and ChatGPT</h3>
<p>Berkeley launches <a href="https://bair.berkeley.edu/blog/2023/04/03/koala/">Koala</a>, a dialogue model trained entirely using freely available data.</p>
<p>They take the crucial step of measuring real human preferences between their model and ChatGPT. While ChatGPT still holds a slight edge, more than 50% of the time users either prefer Koala or have no preference. <strong>Training Cost: $100.</strong></p>
<h3 id="april-15-2023-open-source-rlhf-at-chatgpt-levels">April 15, 2023 - Open Source RLHF at ChatGPT Levels</h3>
<p><a href="https://open-assistant.io/">Open Assistant</a> launches <a href="https://drive.google.com/file/d/10iR5hKwFqAKhL3umx8muOWSRm7hs5FqX/view">a model and, more importantly, a dataset</a> for Alignment via RLHF. Their model is close (48.3% vs. 51.7%) to ChatGPT in terms of human preference. In addition to LLaMA, they show that this dataset can be applied to Pythia-12B, giving people the option to use a fully open stack to run the model. Moreover, because the dataset is publicly available, it takes RLHF from unachievable to cheap and easy for small experimenters.</p>

              </div>
              <div class="toc-container">
                <ul class="markdownIt-TOC">
<li><a href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%B2%A1%E6%9C%89%E5%A3%81%E5%9E%92">大模型没有壁垒</a>
<ul>
<li><a href="#%E5%BC%95%E8%A8%80">引言</a></li>
<li><a href="#complain">Complain</a></li>
<li><a href="#reason">Reason</a></li>
<li><a href="#why">Why</a></li>
<li><a href="#regret">Regret</a><br>
*
<ul>
<li><a href="#lora-is-an-incredibly-powerful-technique-we-should-probably-be-paying-more-attention-to">LoRA is an incredibly powerful technique we should probably be paying more attention to</a></li>
</ul>
</li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#epilogue">Epilogue</a></li>
</ul>
</li>
<li><a href="#the-timeline">The Timeline</a><br>
*
<ul>
<li><a href="#feb-24-2023-llama-is-launched">Feb 24, 2023 - LLaMA is Launched</a></li>
<li><a href="#march-3-2023-the-inevitable-happens">March 3, 2023 - The Inevitable Happens</a></li>
<li><a href="#march-12-2023-language-models-on-a-toaster">March 12, 2023 - Language models on a Toaster</a></li>
<li><a href="#march-13-2023-fine-tuning-on-a-laptop">March 13, 2023 - Fine Tuning on a Laptop</a></li>
<li><a href="#march-18-2023-now-its-fast">March 18, 2023 - Now It’s Fast</a></li>
<li><a href="#march-19-2023-a-13b-model-achieves-parity-with-bard">March 19, 2023 - A 13B model achieves “parity” with Bard</a></li>
<li><a href="#march-25-2023-choose-your-own-model">March 25, 2023 - Choose Your Own Model</a></li>
<li><a href="#march-28-2023-open-source-gpt-3">March 28, 2023 - Open Source GPT-3</a></li>
<li><a href="#march-28-2023-multimodal-training-in-one-hour">March 28, 2023 - Multimodal Training in One Hour</a></li>
<li><a href="#april-3-2023-real-humans-cant-tell-the-difference-between-a-13b-open-model-and-chatgpt">April 3, 2023 - Real Humans Can’t Tell the Difference Between a 13B Open Model and ChatGPT</a></li>
<li><a href="#april-15-2023-open-source-rlhf-at-chatgpt-levels">April 15, 2023 - Open Source RLHF at ChatGPT Levels</a></li>
</ul>
</li>
</ul>

              </div>
            </div>
          </article>
        </div>

        
          <div class="next-post">
            <div class="next">下一篇</div>
            <a href="https://johnyin-hub.github.io/post/ru-he-xun-lian-yi-ge-da-mo-xing/">
              <h3 class="post-title">
                如何训练一个大模型？
              </h3>
            </a>
          </div>
        

        

        <div class="site-footer">
  你我皆凡人，终日奔波苦
  <a class="rss" href="https://johnyin-hub.github.io//atom.xml" target="_blank">
    <i class="ri-rss-line"></i> RSS
  </a>
</div>

      </div>
    </div>

    <script>
      hljs.initHighlightingOnLoad()

      let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

      // This should probably be throttled.
      // Especially because it triggers during smooth scrolling.
      // https://lodash.com/docs/4.17.10#throttle
      // You could do like...
      // window.addEventListener("scroll", () => {
      //    _.throttle(doThatStuff, 100);
      // });
      // Only not doing it here to keep this Pen dependency-free.

      window.addEventListener("scroll", event => {
        let fromTop = window.scrollY;

        mainNavLinks.forEach((link, index) => {
          let section = document.getElementById(decodeURI(link.hash).substring(1));
          let nextSection = null
          if (mainNavLinks[index + 1]) {
            nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
          }
          if (section.offsetTop <= fromTop) {
            if (nextSection) {
              if (nextSection.offsetTop > fromTop) {
                link.classList.add("current");
              } else {
                link.classList.remove("current");    
              }
            } else {
              link.classList.add("current");
            }
          } else {
            link.classList.remove("current");
          }
        });
      });

    </script>
  </body>
</html>
